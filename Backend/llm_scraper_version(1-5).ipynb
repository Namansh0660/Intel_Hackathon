{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "notebook693c315fb3",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-04T20:37:25.521347Z",
          "iopub.execute_input": "2024-10-04T20:37:25.522145Z",
          "iopub.status.idle": "2024-10-04T20:37:39.158204Z",
          "shell.execute_reply.started": "2024-10-04T20:37:25.5221Z",
          "shell.execute_reply": "2024-10-04T20:37:39.156881Z"
        },
        "trusted": true,
        "id": "0itGkKZ-DOnE",
        "outputId": "1b94f67f-0928-44fa-cb42-6c449d42559e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting openai\n  Downloading openai-1.51.0-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai) (4.4.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai) (0.27.0)\nCollecting jiter<1,>=0.4.0 (from openai)\n  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai) (2.9.2)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai) (1.3.1)\nRequirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai) (4.66.4)\nRequirement already satisfied: typing-extensions<5,>=4.11 in /opt/conda/lib/python3.10/site-packages (from openai) (4.12.2)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\nDownloading openai-1.51.0-py3-none-any.whl (383 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.5/383.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: jiter, openai\nSuccessfully installed jiter-0.5.0 openai-1.51.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import random\n",
        "import urllib.parse\n",
        "\n",
        "# Step 1: User Input\n",
        "category = input(\"Enter the category (e.g., real estate): \")\n",
        "location = input(\"Enter the location (e.g., Chennai): \")\n",
        "\n",
        "# Function to print step with timestamps\n",
        "def print_step(step):\n",
        "    print(f\"Step {step}: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Function to generate a Google search query\n",
        "def generate_google_search_query(category, location):\n",
        "    return f\"{category} in {location}\"\n",
        "\n",
        "# Search for websites based on category and location\n",
        "def search_for_websites(category, location):\n",
        "    print_step(1)\n",
        "\n",
        "    # Generate search query for Google\n",
        "    query = generate_google_search_query(category, location)\n",
        "    print(f\"Searching for: {query}\")\n",
        "\n",
        "    # Replace spaces with '+' for the URL\n",
        "    search_url = f\"https://www.google.com/search?q={urllib.parse.quote_plus(query)}\"\n",
        "\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
        "    }\n",
        "\n",
        "    # Send a request to Google\n",
        "    response = requests.get(search_url, headers=headers)\n",
        "\n",
        "    found_websites = []\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        print(\"Successfully accessed Google search results, scraping links...\")\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        # Find links to relevant websites in the search results\n",
        "        for g in soup.find_all('h3'):\n",
        "            link = g.find_parent('a')\n",
        "            if link:\n",
        "                url = link.get('href')\n",
        "                # Clean up the URL and ensure it is valid\n",
        "                if url.startswith('/url?q='):\n",
        "                    url = url[7:url.find('&')]\n",
        "                found_websites.append(url)\n",
        "\n",
        "        # Return only the first 3 unique websites to avoid duplicates\n",
        "        found_websites = list(set(found_websites))[:3]\n",
        "    else:\n",
        "        print(f\"Failed to access Google, status code {response.status_code}\")\n",
        "\n",
        "    print(f\"Found websites: {found_websites}\")\n",
        "    return found_websites\n",
        "\n",
        "# Scraping function for each website\n",
        "def scrape_website_data(url):\n",
        "    print_step(2)\n",
        "    print(f\"Visiting website: {url}\")\n",
        "\n",
        "    try:\n",
        "        # Simulate a visit to the website (Replace with actual scraping logic)\n",
        "        response = requests.get(url)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            print(f\"Successfully accessed {url}, scraping data...\")\n",
        "\n",
        "            # Simulated scraping logic (Use BeautifulSoup for actual HTML parsing)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            email = \"contact@example.com\"  # Placeholder email scraping\n",
        "            phone = \"+91-1234567890\"  # Placeholder phone scraping\n",
        "            social_ids = [\"facebook.com/example\", \"twitter.com/example\"]  # Placeholder social IDs\n",
        "\n",
        "            data = {\n",
        "                \"website\": url,\n",
        "                \"email\": email,\n",
        "                \"phone\": phone,\n",
        "                \"social_ids\": social_ids\n",
        "            }\n",
        "            return data\n",
        "        else:\n",
        "            print(f\"Failed to access {url}, status code {response.status_code}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Main function to handle the process\n",
        "def scrape_business_leads(category, location):\n",
        "    print(\"Starting the scraping process...\")\n",
        "\n",
        "    # Step 1: Search for websites\n",
        "    websites = search_for_websites(category, location)\n",
        "\n",
        "    # Step 2: Scrape each website\n",
        "    leads = []\n",
        "    for site in websites:\n",
        "        print(f\"\\nProcessing {site}...\")\n",
        "        data = scrape_website_data(site)\n",
        "        if data:\n",
        "            leads.append(data)\n",
        "\n",
        "    # Step 3: Display results\n",
        "    print(\"\\nScraping completed. Here are the results:\")\n",
        "    for lead in leads:\n",
        "        print(f\"Website: {lead['website']}\")\n",
        "        print(f\"Email: {lead['email']}\")\n",
        "        print(f\"Phone: {lead['phone']}\")\n",
        "        print(f\"Social IDs: {', '.join(lead['social_ids'])}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "# Running the main function\n",
        "if __name__ == \"__main__\":\n",
        "    scrape_business_leads(category, location)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-04T23:56:06.388316Z",
          "iopub.execute_input": "2024-10-04T23:56:06.388907Z",
          "iopub.status.idle": "2024-10-04T23:56:15.915502Z",
          "shell.execute_reply.started": "2024-10-04T23:56:06.388862Z",
          "shell.execute_reply": "2024-10-04T23:56:15.91429Z"
        },
        "trusted": true,
        "id": "vzIepIn-DOnL",
        "outputId": "2b016da4-1f5e-4059-e77d-8e5c435db236"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "Enter the category (e.g., real estate):  real-estate\nEnter the location (e.g., Chennai):  chennai\n"
        },
        {
          "name": "stdout",
          "text": "Starting the scraping process...\nStep 1: 2024-10-04 23:56:15\nSearching for: real-estate in chennai\nSuccessfully accessed Google search results, scraping links...\nFound websites: ['/url?esrc=s&q=&rct=j&sa=U&url=https://www.realtor.com/international/in/chennai-tamil-nadu//&ved=2ahUKEwiqy4jV9vWIAxVSweYEHeJ0LBQQFnoECAIQAg&usg=AOvVaw17K3V3DjM93Vu2s5Jt3ydt', '/url?esrc=s&q=&rct=j&sa=U&url=https://www.squareyards.com/sale/property-for-sale-in-chennai&ved=2ahUKEwiqy4jV9vWIAxVSweYEHeJ0LBQQFnoECAUQAg&usg=AOvVaw3V2_7V_YekjhDi51DJ6buM', '/url?esrc=s&q=&rct=j&sa=U&url=https://www.chennaiproperties.in/&ved=2ahUKEwiqy4jV9vWIAxVSweYEHeJ0LBQQFnoECAoQAg&usg=AOvVaw3jlCM-MZXVKdJkm_Kc2w2P']\n\nProcessing /url?esrc=s&q=&rct=j&sa=U&url=https://www.realtor.com/international/in/chennai-tamil-nadu//&ved=2ahUKEwiqy4jV9vWIAxVSweYEHeJ0LBQQFnoECAIQAg&usg=AOvVaw17K3V3DjM93Vu2s5Jt3ydt...\nStep 2: 2024-10-04 23:56:15\nVisiting website: /url?esrc=s&q=&rct=j&sa=U&url=https://www.realtor.com/international/in/chennai-tamil-nadu//&ved=2ahUKEwiqy4jV9vWIAxVSweYEHeJ0LBQQFnoECAIQAg&usg=AOvVaw17K3V3DjM93Vu2s5Jt3ydt\nError accessing /url?esrc=s&q=&rct=j&sa=U&url=https://www.realtor.com/international/in/chennai-tamil-nadu//&ved=2ahUKEwiqy4jV9vWIAxVSweYEHeJ0LBQQFnoECAIQAg&usg=AOvVaw17K3V3DjM93Vu2s5Jt3ydt: No connection adapters were found for '/url?esrc=s&q=&rct=j&sa=U&url=https://www.realtor.com/international/in/chennai-tamil-nadu//&ved=2ahUKEwiqy4jV9vWIAxVSweYEHeJ0LBQQFnoECAIQAg&usg=AOvVaw17K3V3DjM93Vu2s5Jt3ydt'\n\nProcessing /url?esrc=s&q=&rct=j&sa=U&url=https://www.squareyards.com/sale/property-for-sale-in-chennai&ved=2ahUKEwiqy4jV9vWIAxVSweYEHeJ0LBQQFnoECAUQAg&usg=AOvVaw3V2_7V_YekjhDi51DJ6buM...\nStep 2: 2024-10-04 23:56:15\nVisiting website: /url?esrc=s&q=&rct=j&sa=U&url=https://www.squareyards.com/sale/property-for-sale-in-chennai&ved=2ahUKEwiqy4jV9vWIAxVSweYEHeJ0LBQQFnoECAUQAg&usg=AOvVaw3V2_7V_YekjhDi51DJ6buM\nError accessing /url?esrc=s&q=&rct=j&sa=U&url=https://www.squareyards.com/sale/property-for-sale-in-chennai&ved=2ahUKEwiqy4jV9vWIAxVSweYEHeJ0LBQQFnoECAUQAg&usg=AOvVaw3V2_7V_YekjhDi51DJ6buM: No connection adapters were found for '/url?esrc=s&q=&rct=j&sa=U&url=https://www.squareyards.com/sale/property-for-sale-in-chennai&ved=2ahUKEwiqy4jV9vWIAxVSweYEHeJ0LBQQFnoECAUQAg&usg=AOvVaw3V2_7V_YekjhDi51DJ6buM'\n\nProcessing /url?esrc=s&q=&rct=j&sa=U&url=https://www.chennaiproperties.in/&ved=2ahUKEwiqy4jV9vWIAxVSweYEHeJ0LBQQFnoECAoQAg&usg=AOvVaw3jlCM-MZXVKdJkm_Kc2w2P...\nStep 2: 2024-10-04 23:56:15\nVisiting website: /url?esrc=s&q=&rct=j&sa=U&url=https://www.chennaiproperties.in/&ved=2ahUKEwiqy4jV9vWIAxVSweYEHeJ0LBQQFnoECAoQAg&usg=AOvVaw3jlCM-MZXVKdJkm_Kc2w2P\nError accessing /url?esrc=s&q=&rct=j&sa=U&url=https://www.chennaiproperties.in/&ved=2ahUKEwiqy4jV9vWIAxVSweYEHeJ0LBQQFnoECAoQAg&usg=AOvVaw3jlCM-MZXVKdJkm_Kc2w2P: No connection adapters were found for '/url?esrc=s&q=&rct=j&sa=U&url=https://www.chennaiproperties.in/&ved=2ahUKEwiqy4jV9vWIAxVSweYEHeJ0LBQQFnoECAoQAg&usg=AOvVaw3jlCM-MZXVKdJkm_Kc2w2P'\n\nScraping completed. Here are the results:\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import urllib.parse\n",
        "import re\n",
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "# Function to print step with timestamps\n",
        "def print_step(step):\n",
        "    print(f\"Step {step}: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# Function to generate a Google search query\n",
        "def generate_google_search_query(category, location):\n",
        "    return f\"{category} in {location}\"\n",
        "\n",
        "# Search for websites based on category and location\n",
        "def search_for_websites(category, location):\n",
        "    print_step(1)\n",
        "\n",
        "    # Generate search query for Google\n",
        "    query = generate_google_search_query(category, location)\n",
        "    print(f\"Searching for: {query}\")\n",
        "\n",
        "    # Replace spaces with '+' for the URL\n",
        "    search_url = f\"https://www.google.com/search?q={urllib.parse.quote_plus(query)}\"\n",
        "\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "\n",
        "    # Send a request to Google\n",
        "    try:\n",
        "        response = requests.get(search_url, headers=headers, timeout=15)\n",
        "\n",
        "        found_websites = []\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            print(\"Successfully accessed Google search results, scraping links...\")\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            # Find all div elements with class 'g'\n",
        "            for result in soup.find_all('div', class_='g'):\n",
        "                link = result.find('a', href=True)\n",
        "                if link:\n",
        "                    href = link['href']\n",
        "                    if href.startswith('http') and 'google.com' not in href:\n",
        "                        found_websites.append(href)\n",
        "\n",
        "            # Return only the first 3 unique websites to avoid duplicates\n",
        "            found_websites = list(set(found_websites))[:3]\n",
        "        else:\n",
        "            print(f\"Failed to access Google, status code {response.status_code}\")\n",
        "\n",
        "        print(f\"Found websites: {found_websites}\")\n",
        "        return found_websites\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error accessing Google search: {e}\")\n",
        "        return []\n",
        "\n",
        "# Improved scraping function with better error handling\n",
        "def scrape_website_data(url):\n",
        "    print_step(2)\n",
        "    print(f\"Visiting website: {url}\")\n",
        "\n",
        "    try:\n",
        "        headers = {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, timeout=15)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            print(f\"Successfully accessed {url}, scraping data...\")\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Email extraction\n",
        "            email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "            emails = re.findall(email_pattern, response.text)\n",
        "            email = emails[0] if emails else \"No email found\"\n",
        "\n",
        "            # Phone extraction\n",
        "            phone_pattern = r'\\b(?:\\+?(\\d{1,3}))?[-. (]*(\\d{3})[-. )]*(\\d{3})[-. ]*(\\d{4})(?: *x\\d+)?\\b'\n",
        "            phones = re.findall(phone_pattern, response.text)\n",
        "            phone = ''.join(phones[0]) if phones else \"No phone found\"\n",
        "\n",
        "            # Social media extraction\n",
        "            social_ids = []\n",
        "            social_patterns = {\n",
        "                'facebook': r'(?:https?:)?\\/\\/(?:www\\.)?facebook\\.com\\/[a-zA-Z0-9.]+\\/?',\n",
        "                'twitter': r'(?:https?:)?\\/\\/(?:www\\.)?twitter\\.com\\/[a-zA-Z0-9_]+\\/?',\n",
        "                'linkedin': r'(?:https?:)?\\/\\/(?:www\\.)?linkedin\\.com\\/(?:company|in)\\/[a-zA-Z0-9\\-]+\\/?'\n",
        "            }\n",
        "            for platform, pattern in social_patterns.items():\n",
        "                matches = re.findall(pattern, response.text)\n",
        "                if matches:\n",
        "                    social_ids.extend(matches)\n",
        "\n",
        "            data = {\n",
        "                \"website\": url,\n",
        "                \"email\": email,\n",
        "                \"phone\": phone,\n",
        "                \"social_ids\": social_ids if social_ids else [\"No social media found\"]\n",
        "            }\n",
        "            return data\n",
        "        else:\n",
        "            print(f\"Failed to access {url}, status code {response.status_code}\")\n",
        "            return None\n",
        "    except requests.Timeout:\n",
        "        print(f\"Timeout error accessing {url}\")\n",
        "        return None\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error accessing {url}: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error processing {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to save data to CSV\n",
        "def save_to_csv(leads, category, location):\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"leads_{category}_{location}_{timestamp}.csv\"\n",
        "\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        fieldnames = ['website', 'email', 'phone', 'social_ids']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "        writer.writeheader()\n",
        "        for lead in leads:\n",
        "            lead_copy = lead.copy()\n",
        "            lead_copy['social_ids'] = ', '.join(lead_copy['social_ids'])  # Convert list to string\n",
        "            writer.writerow(lead_copy)\n",
        "\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "# Main function to handle the process\n",
        "def scrape_business_leads(category, location):\n",
        "    print(\"Starting the scraping process...\")\n",
        "\n",
        "    # Step 1: Search for websites\n",
        "    websites = search_for_websites(category, location)\n",
        "\n",
        "    # Step 2: Scrape each website\n",
        "    leads = []\n",
        "    for site in websites:\n",
        "        print(f\"\\nProcessing {site}...\")\n",
        "        data = scrape_website_data(site)\n",
        "        if data:\n",
        "            leads.append(data)\n",
        "\n",
        "    # Step 3: Display results\n",
        "    print(\"\\nScraping completed. Here are the results:\")\n",
        "    for lead in leads:\n",
        "        print(f\"Website: {lead['website']}\")\n",
        "        print(f\"Email: {lead['email']}\")\n",
        "        print(f\"Phone: {lead['phone']}\")\n",
        "        print(f\"Social IDs: {', '.join(lead['social_ids'])}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    # Step 4: Save results to CSV\n",
        "    if leads:\n",
        "        save_to_csv(leads, category, location)\n",
        "    else:\n",
        "        print(\"No data to save.\")\n",
        "\n",
        "# Running the main function\n",
        "if __name__ == \"__main__\":\n",
        "    category = input(\"Enter the category (e.g., real estate): \")\n",
        "    location = input(\"Enter the location (e.g., Chennai): \")\n",
        "    scrape_business_leads(category, location)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-05T00:18:15.149982Z",
          "iopub.execute_input": "2024-10-05T00:18:15.150392Z",
          "iopub.status.idle": "2024-10-05T00:20:13.361664Z",
          "shell.execute_reply.started": "2024-10-05T00:18:15.150354Z",
          "shell.execute_reply": "2024-10-05T00:20:13.360491Z"
        },
        "trusted": true,
        "id": "YbbBNKHODOnO",
        "outputId": "a2d440c3-31c6-47f4-e301-30bff768c7a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "Enter the category (e.g., real estate):  real estate\nEnter the location (e.g., Chennai):  Chennai\n"
        },
        {
          "name": "stdout",
          "text": "Starting the scraping process...\nStep 1: 2024-10-05 00:19:55\nSearching for: real estate in Chennai\nSuccessfully accessed Google search results, scraping links...\nFound websites: ['https://www.99acres.com/property-in-chennai-ffid', 'https://www.chennaiproperties.in/', 'https://appaswamy.com/']\n\nProcessing https://www.99acres.com/property-in-chennai-ffid...\nStep 2: 2024-10-05 00:19:56\nVisiting website: https://www.99acres.com/property-in-chennai-ffid\nTimeout error accessing https://www.99acres.com/property-in-chennai-ffid\n\nProcessing https://www.chennaiproperties.in/...\nStep 2: 2024-10-05 00:20:11\nVisiting website: https://www.chennaiproperties.in/\nSuccessfully accessed https://www.chennaiproperties.in/, scraping data...\n\nProcessing https://appaswamy.com/...\nStep 2: 2024-10-05 00:20:12\nVisiting website: https://appaswamy.com/\nSuccessfully accessed https://appaswamy.com/, scraping data...\n\nScraping completed. Here are the results:\nWebsite: https://www.chennaiproperties.in/\nEmail: No email found\nPhone: No phone found\nSocial IDs: https://www.facebook.com/chennaiproperties.in/, https://www.facebook.com/pages/, https://twitter.com/no1chproperty, https://twitter.com/no1chproperty, https://www.linkedin.com/company/chennaiproperties, https://www.linkedin.com/company/chennaiproperties\n----------------------------------------\nWebsite: https://appaswamy.com/\nEmail: info@appaswamy.com\nPhone: 10835553217\nSocial IDs: https://www.facebook.com/tr, https://www.facebook.com/appaswamyrealestates/, https://www.linkedin.com/company/appaswamy-real-estates-ltd/\n----------------------------------------\nData saved to leads_real estate_Chennai_20241005_002013.csv\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SNSjSaH9DOnQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}