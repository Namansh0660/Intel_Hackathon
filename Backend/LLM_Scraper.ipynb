{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community ollama selenium beautifulsoup4 requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdAI_GYaD2M7",
        "outputId": "32bfc159-af99-425e-eb8f-34d54b067cc7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting ollama\n",
            "  Downloading ollama-0.3.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.25.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.8)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.1 (from langchain-community)\n",
            "  Downloading langchain-0.3.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.6 (from langchain-community)\n",
            "  Downloading langchain_core-0.3.9-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.125 (from langchain-community)\n",
            "  Downloading langsmith-0.1.131-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-community)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting httpx<0.28.0,>=0.27.0 (from ollama)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.26.2-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.8.30)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx<0.28.0,>=0.27.0->ollama)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain<0.4.0,>=0.3.1->langchain-community)\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.1->langchain-community) (2.9.2)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.6->langchain-community)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain-community) (24.1)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.125->langchain-community)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.125->langchain-community)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.6->langchain-community)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.1->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.1->langchain-community) (2.23.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_community-0.3.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ollama-0.3.3-py3-none-any.whl (10 kB)\n",
            "Downloading selenium-4.25.0-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.2-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.9-py3-none-any.whl (401 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.8/401.8 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.1.131-py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading trio-0.26.2-py3-none-any.whl (475 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.0/476.0 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: tenacity, python-dotenv, outcome, orjson, mypy-extensions, marshmallow, jsonpointer, h11, wsproto, typing-inspect, trio, requests-toolbelt, jsonpatch, httpcore, trio-websocket, pydantic-settings, httpx, dataclasses-json, selenium, ollama, langsmith, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed dataclasses-json-0.6.7 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.2 langchain-community-0.3.1 langchain-core-0.3.9 langchain-text-splitters-0.3.0 langsmith-0.1.131 marshmallow-3.22.0 mypy-extensions-1.0.0 ollama-0.3.3 orjson-3.10.7 outcome-1.3.0.post0 pydantic-settings-2.5.2 python-dotenv-1.0.1 requests-toolbelt-1.0.0 selenium-4.25.0 tenacity-8.5.0 trio-0.26.2 trio-websocket-0.11.1 typing-inspect-0.9.0 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colorama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzrN_pFMEDMV",
        "outputId": "c8295676-29b4-4906-de93-1f9bb4816e2a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama\n",
            "Successfully installed colorama-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sBdxUilkBM4J"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.parse\n",
        "import re\n",
        "import json\n",
        "from typing import List, Dict, Optional\n",
        "from dataclasses import dataclass, field\n",
        "import logging\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from collections import deque\n",
        "from langchain_community.llms import Ollama\n",
        "import colorama\n",
        "from colorama import Fore, Back, Style\n",
        "\n",
        "# Initialize colorama for colored output\n",
        "colorama.init()\n",
        "\n",
        "@dataclass\n",
        "class RealEstateAgent:\n",
        "    name: str = \"\"\n",
        "    title: str = \"\"\n",
        "    company: str = \"\"\n",
        "    email: str = \"\"\n",
        "    phone: str = \"\"\n",
        "    location: str = \"\"\n",
        "    social_media: Dict[str, str] = field(default_factory=dict)\n",
        "    website: str = \"\"\n",
        "    bio: str = \"\"\n",
        "    specialties: List[str] = field(default_factory=list)\n",
        "    languages: List[str] = field(default_factory=list)\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {k: v for k, v in self.__dict__.items() if v}\n",
        "\n",
        "    def __str__(self):\n",
        "        info = []\n",
        "        if self.name:\n",
        "            info.append(f\"Name: {self.name}\")\n",
        "        if self.title:\n",
        "            info.append(f\"Title: {self.title}\")\n",
        "        if self.company:\n",
        "            info.append(f\"Company: {self.company}\")\n",
        "        if self.email:\n",
        "            info.append(f\"Email: {self.email}\")\n",
        "        if self.phone:\n",
        "            info.append(f\"Phone: {self.phone}\")\n",
        "        if self.location:\n",
        "            info.append(f\"Location: {self.location}\")\n",
        "        if self.social_media:\n",
        "            info.append(\"Social Media:\")\n",
        "            for platform, url in self.social_media.items():\n",
        "                info.append(f\"  - {platform}: {url}\")\n",
        "        if self.specialties:\n",
        "            info.append(f\"Specialties: {', '.join(self.specialties)}\")\n",
        "        if self.languages:\n",
        "            info.append(f\"Languages: {', '.join(self.languages)}\")\n",
        "        if self.bio:\n",
        "            info.append(f\"Bio: {self.bio[:200]}...\")\n",
        "        return \"\\n\".join(info)\n",
        "\n",
        "class VerboseRealEstateLeadScraper:\n",
        "    def __init__(self, max_depth: int = 1, max_results: int = 5, model_name: str = \"llama2\"):\n",
        "        self.max_depth = max_depth\n",
        "        self.max_results = max_results\n",
        "        self.llm = Ollama(model=model_name)\n",
        "        self.visited_urls = set()\n",
        "        self.visited_domains = set()\n",
        "        self.setup_logging()\n",
        "        self.setup_patterns()\n",
        "        self.headers = {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "        }\n",
        "\n",
        "    def setup_logging(self):\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
        "        )\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def setup_patterns(self):\n",
        "        self.patterns = {\n",
        "            'email': re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'),\n",
        "            'phone': re.compile(r'(?:\\+?1[-.]?)?\\s*(?:\\([0-9]{3}\\)|[0-9]{3})[-.]?[0-9]{3}[-.]?[0-9]{4}'),\n",
        "            'social': {\n",
        "                'linkedin': re.compile(r'linkedin\\.com/(?:in|company)/[a-zA-Z0-9_-]+'),\n",
        "                'facebook': re.compile(r'facebook\\.com/[a-zA-Z0-9.]+'),\n",
        "                'instagram': re.compile(r'instagram\\.com/[a-zA-Z0-9_]+'),\n",
        "                'twitter': re.compile(r'twitter\\.com/[a-zA-Z0-9_]+')\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def print_debug(self, message: str, color: str = Fore.WHITE, prefix: str = \"\"):\n",
        "        timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        print(f\"{color}[{timestamp}] {prefix}{message}{Style.RESET_ALL}\")\n",
        "\n",
        "    def get_base_domain(self, url: str) -> str:\n",
        "        \"\"\"Extract the base domain from a URL.\"\"\"\n",
        "        parsed_url = urlparse(url)\n",
        "        return parsed_url.netloc\n",
        "\n",
        "    def google_search(self, query: str) -> List[Dict]:\n",
        "        self.print_debug(f\"Performing Google search for: {query}\", Fore.CYAN, \"🔍 \")\n",
        "        query_encoded = urllib.parse.quote_plus(query)\n",
        "        url = f\"https://www.google.com/search?q={query_encoded}\"\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, headers=self.headers)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            search_results = soup.find_all('div', class_='g')\n",
        "\n",
        "            results = []\n",
        "            for result in search_results:\n",
        "                title_element = result.find('h3')\n",
        "                link_element = result.find('a')\n",
        "\n",
        "                if title_element and link_element:\n",
        "                    results.append({\n",
        "                        'title': title_element.get_text(),\n",
        "                        'link': link_element['href']\n",
        "                    })\n",
        "                    self.print_debug(f\"Found result: {title_element.get_text()}\", Fore.GREEN, \"  ✓ \")\n",
        "\n",
        "            self.print_debug(f\"Found {len(results)} search results\", Fore.CYAN, \"📊 \")\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            self.print_debug(f\"Error in Google search: {str(e)}\", Fore.RED, \"❌ \")\n",
        "            return []\n",
        "\n",
        "    def get_internal_links(self, url: str, html_content: str) -> List[str]:\n",
        "        self.print_debug(f\"Scanning for internal links on: {url}\", Fore.YELLOW, \"🔍 \")\n",
        "        try:\n",
        "            soup = BeautifulSoup(html_content, 'html.parser')\n",
        "            base_domain = self.get_base_domain(url)\n",
        "            internal_links = set()\n",
        "\n",
        "            relevant_patterns = [\n",
        "                '/agent/', '/about/', '/team/', '/contact/',\n",
        "                '/profile/', '/bio/', '/realtor/', '/broker/'\n",
        "            ]\n",
        "\n",
        "            for link in soup.find_all('a', href=True):\n",
        "                href = link['href']\n",
        "                absolute_url = urljoin(url, href)\n",
        "\n",
        "                if self.get_base_domain(absolute_url) == base_domain:\n",
        "                    if any(pattern in absolute_url.lower() for pattern in relevant_patterns):\n",
        "                        internal_links.add(absolute_url)\n",
        "                        self.print_debug(f\"Found relevant internal link: {absolute_url}\", Fore.GREEN, \"  ✓ \")\n",
        "\n",
        "            self.print_debug(f\"Found {len(internal_links)} relevant internal links\", Fore.YELLOW, \"📊 \")\n",
        "            return list(internal_links)\n",
        "        except Exception as e:\n",
        "            self.print_debug(f\"Error getting internal links: {str(e)}\", Fore.RED, \"❌ \")\n",
        "            return []\n",
        "\n",
        "    def extract_agent_info(self, url: str, html_content: str) -> Optional[RealEstateAgent]:\n",
        "        self.print_debug(f\"Extracting agent information from: {url}\", Fore.MAGENTA, \"🔍 \")\n",
        "        try:\n",
        "            soup = BeautifulSoup(html_content, 'html.parser')\n",
        "            text_content = ' '.join(soup.stripped_strings)\n",
        "\n",
        "            agent = RealEstateAgent()\n",
        "            agent.website = url\n",
        "\n",
        "            # Extract emails\n",
        "            emails = self.patterns['email'].findall(text_content)\n",
        "            if emails:\n",
        "                agent.email = emails[0]\n",
        "                self.print_debug(f\"Found email: {agent.email}\", Fore.GREEN, \"  ✓ \")\n",
        "\n",
        "            # Extract phone numbers\n",
        "            phones = self.patterns['phone'].findall(text_content)\n",
        "            if phones:\n",
        "                phone = re.sub(r'[^\\d]', '', phones[0])\n",
        "                if len(phone) == 10:\n",
        "                    agent.phone = f\"({phone[:3]}) {phone[3:6]}-{phone[6:]}\"\n",
        "                    self.print_debug(f\"Found phone: {agent.phone}\", Fore.GREEN, \"  ✓ \")\n",
        "\n",
        "            # Extract social media\n",
        "            for platform, pattern in self.patterns['social'].items():\n",
        "                matches = pattern.findall(html_content)\n",
        "                if matches:\n",
        "                    agent.social_media[platform] = f\"https://www.{matches[0]}\"\n",
        "                    self.print_debug(f\"Found {platform} profile\", Fore.GREEN, \"  ✓ \")\n",
        "\n",
        "            # Use LLM for detailed information\n",
        "            self.print_debug(\"Using LLM to extract detailed information...\", Fore.BLUE, \"🤖 \")\n",
        "            prompt = f\"\"\"\n",
        "            Extract real estate agent information from this text:\n",
        "            {text_content[:2000]}\n",
        "\n",
        "            Return a JSON object with these fields:\n",
        "            - name (full name of the agent)\n",
        "            - title (professional title/role)\n",
        "            - company (brokerage or company name)\n",
        "            - location (city and state)\n",
        "            - bio (brief professional description)\n",
        "            - specialties (list of specializations)\n",
        "            - languages (list of languages spoken)\n",
        "\n",
        "            Only include information that is clearly present in the text.\n",
        "            \"\"\"\n",
        "\n",
        "            try:\n",
        "                llm_response = self.llm.invoke(prompt)\n",
        "                agent_info = json.loads(llm_response)\n",
        "\n",
        "                agent.name = agent_info.get('name', '')\n",
        "                agent.title = agent_info.get('title', '')\n",
        "                agent.company = agent_info.get('company', '')\n",
        "                agent.location = agent_info.get('location', '')\n",
        "                agent.bio = agent_info.get('bio', '')\n",
        "                agent.specialties = agent_info.get('specialties', [])\n",
        "                agent.languages = agent_info.get('languages', [])\n",
        "\n",
        "                self.print_debug(\"Successfully extracted agent information\", Fore.GREEN, \"✓ \")\n",
        "\n",
        "            except Exception as e:\n",
        "                self.print_debug(f\"Error parsing LLM response: {str(e)}\", Fore.RED, \"❌ \")\n",
        "\n",
        "            if self.validate_agent(agent):\n",
        "                self.print_debug(\"Agent information validated successfully.\", Fore.GREEN, \"✅ \")\n",
        "                return agent\n",
        "            else:\n",
        "                self.print_debug(\"Agent information validation failed.\", Fore.RED, \"❌ \")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            self.print_debug(f\"Error extracting agent info: {str(e)}\", Fore.RED, \"❌ \")\n",
        "            return None\n",
        "\n",
        "    def validate_agent(self, agent: RealEstateAgent) -> bool:\n",
        "        # Check if mandatory fields are filled\n",
        "        return bool(agent.name and (agent.email or agent.phone))\n",
        "\n",
        "    def scrape_leads(self, query: str) -> List[Dict]:\n",
        "        leads = []\n",
        "        search_results = self.google_search(query)[:self.max_results]\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "            futures = []\n",
        "            for result in search_results:\n",
        "                url = result['link']\n",
        "                domain = self.get_base_domain(url)\n",
        "                if domain not in self.visited_domains:\n",
        "                    self.visited_domains.add(domain)\n",
        "                    self.visited_urls.add(url)\n",
        "                    futures.append(executor.submit(self.process_url, url, depth=1))\n",
        "\n",
        "            for future in as_completed(futures):\n",
        "                agent = future.result()\n",
        "                if agent:\n",
        "                    leads.append(agent.to_dict())\n",
        "\n",
        "        return leads\n",
        "\n",
        "    def process_url(self, url: str, depth: int) -> Optional[RealEstateAgent]:\n",
        "        if depth > self.max_depth:\n",
        "            return None\n",
        "\n",
        "        self.print_debug(f\"Processing URL: {url} (Depth: {depth})\", Fore.YELLOW, \"🔍 \")\n",
        "        try:\n",
        "            response = requests.get(url, headers=self.headers)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            agent_info = self.extract_agent_info(url, response.text)\n",
        "\n",
        "            if not agent_info and depth < self.max_depth:\n",
        "                internal_links = self.get_internal_links(url, response.text)\n",
        "                for link in internal_links:\n",
        "                    if link not in self.visited_urls:\n",
        "                        link_domain = self.get_base_domain(link)\n",
        "                        if link_domain not in self.visited_domains:\n",
        "                            self.visited_urls.add(link)\n",
        "                            internal_agent_info = self.process_url(link, depth + 1)\n",
        "                            if internal_agent_info:\n",
        "                                return internal_agent_info\n",
        "\n",
        "            return agent_info\n",
        "        except Exception as e:\n",
        "            self.print_debug(f\"Error processing URL {url}: {str(e)}\", Fore.RED, \"❌ \")\n",
        "            return None\n",
        "\n",
        "    def print_results(self, leads: List[Dict]):\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"{Fore.CYAN}LEAD GENERATION RESULTS{Style.RESET_ALL}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        for i, lead in enumerate(leads, 1):\n",
        "            print(f\"\\n{Fore.GREEN}Lead #{i}{Style.RESET_ALL}\")\n",
        "            print(\"-\"*30)\n",
        "            for key, value in lead.items():\n",
        "                print(f\"{key.capitalize()}: {value}\")\n",
        "\n",
        "        # Optionally, save the results to a JSON file\n",
        "        self.save_results_to_json(leads)\n",
        "\n",
        "    def save_results_to_json(self, leads: List[Dict], filename: str = 'real_estate_leads.json'):\n",
        "        try:\n",
        "            with open(filename, 'w') as f:\n",
        "                json.dump(leads, f, indent=4)\n",
        "            self.print_debug(f\"Results saved to {filename}\", Fore.BLUE, \"💾 \")\n",
        "        except Exception as e:\n",
        "            self.print_debug(f\"Error saving results: {str(e)}\", Fore.RED, \"❌ \")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search_query = \"real estate agents in San Francisco\"\n",
        "scraper = VerboseRealEstateLeadScraper(max_depth=1, max_results=20)\n",
        "leads = scraper.scrape_leads(search_query)\n",
        "scraper.print_results(leads)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrGSCiB5Bj0W",
        "outputId": "96c76758-c575-4a3d-b9dd-572d51437235"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-10-05 00:27:35] 🔍 Performing Google search for: real estate agents in San Francisco\n",
            "[2024-10-05 00:27:36]   ✓ Found result: Find Realtors & Real Estate Agents in San Francisco, CA\n",
            "[2024-10-05 00:27:36]   ✓ Found result: Real estate agents in San Francisco, CA - Zillow\n",
            "[2024-10-05 00:27:36]   ✓ Found result: San Francisco Agents, 873 Compass Real Estate Agents\n",
            "[2024-10-05 00:27:36]   ✓ Found result: Kinoko Real Estate: Top Producing San Francisco Realtors\n",
            "[2024-10-05 00:27:36]   ✓ Found result: 884 Real Estate Agents in San Francisco, CA\n",
            "[2024-10-05 00:27:36]   ✓ Found result: THE BEST 10 Real Estate Agents in SAN FRANCISCO, CA\n",
            "[2024-10-05 00:27:36]   ✓ Found result: SF Real Estate Agent | Realtor San Francisco\n",
            "[2024-10-05 00:27:36]   ✓ Found result: Realtors, San Francisco | Real Estate Agents\n",
            "[2024-10-05 00:27:36]   ✓ Found result: Search for a Real Estate Agent in San Francisco, CA\n",
            "[2024-10-05 00:27:36] 📊 Found 9 search results\n",
            "[2024-10-05 00:27:36] 🔍 Processing URL: https://www.realtor.com/realestateagents/san-francisco_ca (Depth: 1)[2024-10-05 00:27:36] 🔍 Processing URL: https://www.zillow.com/professionals/real-estate-agent-reviews/san-francisco-ca/ (Depth: 1)\n",
            "\n",
            "[2024-10-05 00:27:36] 🔍 Processing URL: https://www.compass.com/agents/locations/san-francisco-ca/44474/ (Depth: 1)\n",
            "[2024-10-05 00:27:36] 🔍 Processing URL: https://kinokorealestate.com/ (Depth: 1)[2024-10-05 00:27:36] 🔍 Processing URL: https://www.coldwellbanker.com/city/ca/san-francisco/agents (Depth: 1)\n",
            "\n",
            "[2024-10-05 00:27:36] 🔍 Extracting agent information from: https://kinokorealestate.com/\n",
            "[2024-10-05 00:27:36]   ✓ Found linkedin profile[2024-10-05 00:27:36] ❌ Error processing URL https://www.zillow.com/professionals/real-estate-agent-reviews/san-francisco-ca/: 403 Client Error: Forbidden for url: https://www.zillow.com/professionals/real-estate-agent-reviews/san-francisco-ca/\n",
            "\n",
            "[2024-10-05 00:27:36] 🔍 Processing URL: https://www.yelp.com/search?cflt=realestateagents&find_loc=San+Francisco%2C+CA (Depth: 1)[2024-10-05 00:27:36]   ✓ Found facebook profile\n",
            "\n",
            "[2024-10-05 00:27:36]   ✓ Found instagram profile\n",
            "[2024-10-05 00:27:36] 🤖 Using LLM to extract detailed information...\n",
            "[2024-10-05 00:27:36] ❌ Error parsing LLM response: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7bdfc1471240>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "[2024-10-05 00:27:36] ❌ Agent information validation failed.\n",
            "[2024-10-05 00:27:36] 🔍 Processing URL: https://ruthkrishnan.com/ (Depth: 1)\n",
            "[2024-10-05 00:27:36] ❌ Error processing URL https://www.realtor.com/realestateagents/san-francisco_ca: 403 Client Error: Forbidden for url: https://www.realtor.com/realestateagents/san-francisco_ca\n",
            "[2024-10-05 00:27:36] 🔍 Processing URL: https://cityrealestatesf.com/ (Depth: 1)\n",
            "[2024-10-05 00:27:36] ❌ Error processing URL https://www.yelp.com/search?cflt=realestateagents&find_loc=San+Francisco%2C+CA: 403 Client Error: Forbidden for url: https://www.yelp.com/search?cflt=realestateagents&find_loc=San+Francisco%2C+CA\n",
            "[2024-10-05 00:27:36] 🔍 Processing URL: https://www.century21.com/real-estate-agents/san-francisco-ca/LCCASANFRANCISCO/ (Depth: 1)\n",
            "[2024-10-05 00:27:36] 🔍 Extracting agent information from: https://www.compass.com/agents/locations/san-francisco-ca/44474/\n",
            "[2024-10-05 00:27:36]   ✓ Found email: rachel.abrahampollard@compass.com\n",
            "[2024-10-05 00:27:36]   ✓ Found phone: (415) 828-3964\n",
            "[2024-10-05 00:27:36]   ✓ Found facebook profile\n",
            "[2024-10-05 00:27:36]   ✓ Found instagram profile\n",
            "[2024-10-05 00:27:36]   ✓ Found twitter profile\n",
            "[2024-10-05 00:27:36] 🤖 Using LLM to extract detailed information...\n",
            "[2024-10-05 00:27:36] ❌ Error parsing LLM response: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7bdfc1532530>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "[2024-10-05 00:27:36] ❌ Agent information validation failed.\n",
            "[2024-10-05 00:27:36] 🔍 Extracting agent information from: https://www.century21.com/real-estate-agents/san-francisco-ca/LCCASANFRANCISCO/\n",
            "[2024-10-05 00:27:36]   ✓ Found phone: (866) 732-6139\n",
            "[2024-10-05 00:27:36]   ✓ Found facebook profile\n",
            "[2024-10-05 00:27:36]   ✓ Found instagram profile\n",
            "[2024-10-05 00:27:36]   ✓ Found twitter profile\n",
            "[2024-10-05 00:27:36] 🤖 Using LLM to extract detailed information...\n",
            "[2024-10-05 00:27:36] ❌ Error parsing LLM response: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7bdfc133af80>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "[2024-10-05 00:27:36] ❌ Agent information validation failed.\n",
            "[2024-10-05 00:27:36] 🔍 Extracting agent information from: https://ruthkrishnan.com/\n",
            "[2024-10-05 00:27:36] 🔍 Extracting agent information from: https://cityrealestatesf.com/[2024-10-05 00:27:36]   ✓ Found email: info@ruthkrishnan.com\n",
            "\n",
            "[2024-10-05 00:27:36]   ✓ Found linkedin profile\n",
            "[2024-10-05 00:27:36]   ✓ Found facebook profile\n",
            "[2024-10-05 00:27:37]   ✓ Found instagram profile\n",
            "[2024-10-05 00:27:37] 🤖 Using LLM to extract detailed information...[2024-10-05 00:27:37]   ✓ Found linkedin profile\n",
            "\n",
            "[2024-10-05 00:27:37]   ✓ Found facebook profile[2024-10-05 00:27:37] ❌ Error parsing LLM response: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7bdfc1470160>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "\n",
            "[2024-10-05 00:27:37]   ✓ Found instagram profile[2024-10-05 00:27:37] ❌ Agent information validation failed.\n",
            "\n",
            "[2024-10-05 00:27:37] 🤖 Using LLM to extract detailed information...\n",
            "[2024-10-05 00:27:37] ❌ Error parsing LLM response: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7bdfc1e61ff0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "[2024-10-05 00:27:37] ❌ Agent information validation failed.\n",
            "[2024-10-05 00:27:37] 🔍 Extracting agent information from: https://www.coldwellbanker.com/city/ca/san-francisco/agents\n",
            "[2024-10-05 00:27:37]   ✓ Found linkedin profile\n",
            "[2024-10-05 00:27:37]   ✓ Found facebook profile\n",
            "[2024-10-05 00:27:37]   ✓ Found instagram profile\n",
            "[2024-10-05 00:27:37] 🤖 Using LLM to extract detailed information...\n",
            "[2024-10-05 00:27:37] ❌ Error parsing LLM response: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7bdfc21ed0f0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "[2024-10-05 00:27:37] ❌ Agent information validation failed.\n",
            "\n",
            "==================================================\n",
            "LEAD GENERATION RESULTS\n",
            "==================================================\n",
            "[2024-10-05 00:27:37] 💾 Results saved to real_estate_leads.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.parse\n",
        "import re\n",
        "import json\n",
        "from typing import List, Dict, Optional\n",
        "from dataclasses import dataclass, field\n",
        "import logging\n",
        "from collections import deque\n",
        "import time\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from langchain_community.llms import Ollama\n",
        "import colorama\n",
        "from colorama import Fore, Style\n",
        "\n",
        "# Initialize colorama for colored output\n",
        "colorama.init()\n",
        "\n",
        "@dataclass\n",
        "class RealEstateAgent:\n",
        "    name: str = \"\"\n",
        "    title: str = \"\"\n",
        "    company: str = \"\"\n",
        "    email: str = \"\"\n",
        "    phone: str = \"\"\n",
        "    location: str = \"\"\n",
        "    social_media: Dict[str, str] = field(default_factory=dict)\n",
        "    website: str = \"\"\n",
        "    bio: str = \"\"\n",
        "    specialties: List[str] = field(default_factory=list)\n",
        "    languages: List[str] = field(default_factory=list)\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {k: v for k, v in self.__dict__.items() if v}\n",
        "\n",
        "    def __str__(self):\n",
        "        info = []\n",
        "        if self.name:\n",
        "            info.append(f\"Name: {self.name}\")\n",
        "        if self.title:\n",
        "            info.append(f\"Title: {self.title}\")\n",
        "        if self.company:\n",
        "            info.append(f\"Company: {self.company}\")\n",
        "        if self.email:\n",
        "            info.append(f\"Email: {self.email}\")\n",
        "        if self.phone:\n",
        "            info.append(f\"Phone: {self.phone}\")\n",
        "        if self.location:\n",
        "            info.append(f\"Location: {self.location}\")\n",
        "        if self.social_media:\n",
        "            info.append(\"Social Media:\")\n",
        "            for platform, url in self.social_media.items():\n",
        "                info.append(f\"  - {platform}: {url}\")\n",
        "        if self.specialties:\n",
        "            info.append(f\"Specialties: {', '.join(self.specialties)}\")\n",
        "        if self.languages:\n",
        "            info.append(f\"Languages: {', '.join(self.languages)}\")\n",
        "        if self.bio:\n",
        "            info.append(f\"Bio: {self.bio[:200]}...\")\n",
        "        return \"\\n\".join(info)\n",
        "\n",
        "class VerboseRealEstateLeadScraper:\n",
        "    def __init__(self, max_depth: int = 1, max_results: int = 5, model_name: str = \"llama2\"):\n",
        "        self.max_depth = max_depth\n",
        "        self.max_results = max_results\n",
        "        self.llm = Ollama(model=model_name)\n",
        "        self.visited_urls = set()\n",
        "        self.setup_logging()\n",
        "        self.setup_patterns()\n",
        "        self.headers = {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "        }\n",
        "\n",
        "    def setup_logging(self):\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
        "        )\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def setup_patterns(self):\n",
        "        self.patterns = {\n",
        "            'email': re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'),\n",
        "            'phone': re.compile(r'(?:\\+?1[-.]?)?\\s*(?:\\([0-9]{3}\\)|[0-9]{3})[-.]?[0-9]{3}[-.]?[0-9]{4}'),\n",
        "            'social': {\n",
        "                'linkedin': re.compile(r'linkedin\\.com/(?:in|company)/[a-zA-Z0-9_-]+'),\n",
        "                'facebook': re.compile(r'facebook\\.com/[a-zA-Z0-9.]+'),\n",
        "                'instagram': re.compile(r'instagram\\.com/[a-zA-Z0-9_]+'),\n",
        "                'twitter': re.compile(r'twitter\\.com/[a-zA-Z0-9_]+')\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def print_debug(self, message: str, color: str = Fore.WHITE, prefix: str = \"\"):\n",
        "        timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        print(f\"{color}[{timestamp}] {prefix}{message}{Style.RESET_ALL}\")\n",
        "\n",
        "    def google_search(self, query: str) -> List[str]:\n",
        "        self.print_debug(f\"Performing Google search for: {query}\", Fore.CYAN, \"🔍 \")\n",
        "        query_encoded = urllib.parse.quote_plus(query)\n",
        "        url = f\"https://www.google.com/search?q={query_encoded}\"\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, headers=self.headers)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            search_results = soup.find_all('div', class_='g')\n",
        "\n",
        "            results = []\n",
        "            for result in search_results:\n",
        "                link_element = result.find('a')\n",
        "\n",
        "                if link_element:\n",
        "                    link = link_element['href']\n",
        "                    results.append(link)\n",
        "                    self.print_debug(f\"Found result: {link}\", Fore.GREEN, \"  ✓ \")\n",
        "\n",
        "            self.print_debug(f\"Found {len(results)} search results\", Fore.CYAN, \"📊 \")\n",
        "            return results[:self.max_results]\n",
        "        except Exception as e:\n",
        "            self.print_debug(f\"Error in Google search: {str(e)}\", Fore.RED, \"❌ \")\n",
        "            return []\n",
        "\n",
        "    def get_base_domain(self, url: str) -> str:\n",
        "        \"\"\"Extract the base domain from a URL.\"\"\"\n",
        "        parsed_url = urlparse(url)\n",
        "        return parsed_url.netloc\n",
        "\n",
        "    def get_internal_links(self, url: str, html_content: str) -> List[str]:\n",
        "        self.print_debug(f\"Scanning for internal links on: {url}\", Fore.YELLOW, \"🔍 \")\n",
        "        try:\n",
        "            soup = BeautifulSoup(html_content, 'html.parser')\n",
        "            base_domain = self.get_base_domain(url)\n",
        "            internal_links = set()\n",
        "\n",
        "            relevant_patterns = [\n",
        "                '/agent/', '/about/', '/team/', '/contact/',\n",
        "                '/profile/', '/bio/', '/realtor/', '/broker/'\n",
        "            ]\n",
        "\n",
        "            for link in soup.find_all('a', href=True):\n",
        "                href = link['href']\n",
        "                absolute_url = urljoin(url, href)\n",
        "\n",
        "                if self.get_base_domain(absolute_url) == base_domain:\n",
        "                    if any(pattern in absolute_url.lower() for pattern in relevant_patterns):\n",
        "                        internal_links.add(absolute_url)\n",
        "                        self.print_debug(f\"Found relevant internal link: {absolute_url}\", Fore.GREEN, \"  ✓ \")\n",
        "\n",
        "            self.print_debug(f\"Found {len(internal_links)} relevant internal links\", Fore.YELLOW, \"📊 \")\n",
        "            return list(internal_links)\n",
        "        except Exception as e:\n",
        "            self.print_debug(f\"Error getting internal links: {str(e)}\", Fore.RED, \"❌ \")\n",
        "            return []\n",
        "\n",
        "    def extract_agent_info(self, url: str, html_content: str) -> Optional[RealEstateAgent]:\n",
        "        self.print_debug(f\"Extracting agent information from: {url}\", Fore.MAGENTA, \"🔍 \")\n",
        "        try:\n",
        "            soup = BeautifulSoup(html_content, 'html.parser')\n",
        "            text_content = ' '.join(soup.stripped_strings)\n",
        "\n",
        "            agent = RealEstateAgent()\n",
        "            agent.website = url\n",
        "\n",
        "            # Extract emails\n",
        "            emails = self.patterns['email'].findall(text_content)\n",
        "            if emails:\n",
        "                agent.email = emails[0]\n",
        "                self.print_debug(f\"Found email: {agent.email}\", Fore.GREEN, \"  ✓ \")\n",
        "\n",
        "            # Extract phone numbers\n",
        "            phones = self.patterns['phone'].findall(text_content)\n",
        "            if phones:\n",
        "                phone = re.sub(r'[^\\d]', '', phones[0])\n",
        "                if len(phone) == 10:\n",
        "                    agent.phone = f\"({phone[:3]}) {phone[3:6]}-{phone[6:]}\"\n",
        "                    self.print_debug(f\"Found phone: {agent.phone}\", Fore.GREEN, \"  ✓ \")\n",
        "\n",
        "            # Extract social media\n",
        "            for platform, pattern in self.patterns['social'].items():\n",
        "                matches = pattern.findall(html_content)\n",
        "                if matches:\n",
        "                    agent.social_media[platform] = f\"https://www.{matches[0]}\"\n",
        "                    self.print_debug(f\"Found {platform} profile\", Fore.GREEN, \"  ✓ \")\n",
        "\n",
        "            # Use LLM for detailed information\n",
        "            self.print_debug(\"Using LLM to extract detailed information...\", Fore.BLUE, \"🤖 \")\n",
        "            prompt = f\"\"\"\n",
        "            Extract real estate agent information from this text:\n",
        "            {text_content[:2000]}\n",
        "\n",
        "            Return a JSON object with these fields:\n",
        "            - name (full name of the agent)\n",
        "            - title (professional title/role)\n",
        "            - company (brokerage or company name)\n",
        "            - location (city and state)\n",
        "            - bio (brief professional description)\n",
        "            - specialties (list of specializations)\n",
        "            - languages (list of languages spoken)\n",
        "\n",
        "            Only include information that is clearly present in the text.\n",
        "            \"\"\"\n",
        "\n",
        "            try:\n",
        "                llm_response = self.llm.invoke(prompt)\n",
        "                agent_info = json.loads(llm_response)\n",
        "\n",
        "                agent.name = agent_info.get('name', '')\n",
        "                agent.title = agent_info.get('title', '')\n",
        "                agent.company = agent_info.get('company', '')\n",
        "                agent.location = agent_info.get('location', '')\n",
        "                agent.bio = agent_info.get('bio', '')\n",
        "                agent.specialties = agent_info.get('specialties', [])\n",
        "                agent.languages = agent_info.get('languages', [])\n",
        "\n",
        "                self.print_debug(f\"Extracted agent information: {agent}\", Fore.GREEN, \"  ✓ \")\n",
        "                return agent\n",
        "            except Exception as e:\n",
        "                self.print_debug(f\"LLM error: {str(e)}\", Fore.RED, \"❌ \")\n",
        "                return agent\n",
        "\n",
        "        except Exception as e:\n",
        "            self.print_debug(f\"Error extracting agent info: {str(e)}\", Fore.RED, \"❌ \")\n",
        "            return None\n",
        "\n",
        "    def scrape(self, start_url: str) -> List[RealEstateAgent]:\n",
        "        self.print_debug(f\"Starting scrape at: {start_url}\", Fore.CYAN, \"🌐 \")\n",
        "        agents = []\n",
        "        to_visit = deque([(start_url, 0)])\n",
        "\n",
        "        while to_visit:\n",
        "            url, depth = to_visit.popleft()\n",
        "            if depth > self.max_depth or url in self.visited_urls:\n",
        "                continue\n",
        "\n",
        "            self.visited_urls.add(url)\n",
        "            try:\n",
        "                response = requests.get(url, headers=self.headers)\n",
        "                response.raise_for_status()\n",
        "                html_content = response.text\n",
        "\n",
        "                agents_info = self.extract_agent_info(url, html_content)\n",
        "                if agents_info:\n",
        "                    agents.append(agents_info)\n",
        "\n",
        "                internal_links = self.get_internal_links(url, html_content)\n",
        "                for link in internal_links:\n",
        "                    if link not in self.visited_urls:\n",
        "                        to_visit.append((link, depth + 1))\n",
        "            except Exception as e:\n",
        "                self.print_debug(f\"Error visiting {url}: {str(e)}\", Fore.RED, \"❌ \")\n",
        "\n",
        "        self.print_debug(f\"Scraping completed. Found {len(agents)} agents.\", Fore.CYAN, \"🏁 \")\n",
        "        return agents\n",
        "\n",
        "def main():\n",
        "    query = input(\"Enter search query for real estate agents: \")\n",
        "    scraper = VerboseRealEstateLeadScraper(model_name=\"llama2\")\n",
        "\n",
        "    # Perform Google search and scrape the results\n",
        "    search_results = scraper.google_search(query)\n",
        "\n",
        "    all_agents = []\n",
        "    for result_url in search_results:\n",
        "        agents = scraper.scrape(result_url)\n",
        "        all_agents.extend(agents)\n",
        "\n",
        "    print(\"\\nExtracted Agents:\")\n",
        "    for agent in all_agents:\n",
        "        print(agent)\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cs_6xA8KDopi",
        "outputId": "516205b7-be41-484e-bdb7-7893599d95d1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter search query for real estate agents: real estate people linkedin\n",
            "[2024-10-05 00:33:16] 🔍 Performing Google search for: real estate people linkedin\n",
            "[2024-10-05 00:33:17]   ✓ Found result: https://www.linkedin.com/services/l2/real-estate-agents\n",
            "[2024-10-05 00:33:17]   ✓ Found result: https://www.linkedin.com/company/real-estate-people\n",
            "[2024-10-05 00:33:17]   ✓ Found result: https://www.linkedin.com/pub/dir/Real+Estate/Agent\n",
            "[2024-10-05 00:33:17]   ✓ Found result: https://www.linkedin.com/pub/dir/Real/Estate\n",
            "[2024-10-05 00:33:17]   ✓ Found result: https://www.paperlesspipeline.com/blog/linkedin-real-estate-marketing\n",
            "[2024-10-05 00:33:17]   ✓ Found result: https://www.linkedin.com/company/realtor\n",
            "[2024-10-05 00:33:17]   ✓ Found result: https://www.linkedin.com/pub/dir/Top/Real+Estate\n",
            "[2024-10-05 00:33:17]   ✓ Found result: https://www.linkedin.com/in/billgassett\n",
            "[2024-10-05 00:33:17]   ✓ Found result: https://www.youtube.com/watch?v=HS3QuQHPtgU\n",
            "[2024-10-05 00:33:17]   ✓ Found result: https://www.linkedin.com/services/cl2/real-estate-agents-las-vegas-nevada\n",
            "[2024-10-05 00:33:17] 📊 Found 10 search results\n",
            "[2024-10-05 00:33:17] 🌐 Starting scrape at: https://www.linkedin.com/services/l2/real-estate-agents\n",
            "[2024-10-05 00:33:18] 🔍 Extracting agent information from: https://www.linkedin.com/services/l2/real-estate-agents\n",
            "[2024-10-05 00:33:18]   ✓ Found phone: (958) 200-0415\n",
            "[2024-10-05 00:33:18]   ✓ Found linkedin profile\n",
            "[2024-10-05 00:33:18] 🤖 Using LLM to extract detailed information...\n",
            "[2024-10-05 00:33:18] ❌ LLM error: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7bdfc24fb3a0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "[2024-10-05 00:33:18] 🔍 Scanning for internal links on: https://www.linkedin.com/services/l2/real-estate-agents\n",
            "[2024-10-05 00:33:18] 📊 Found 0 relevant internal links\n",
            "[2024-10-05 00:33:18] 🏁 Scraping completed. Found 1 agents.\n",
            "[2024-10-05 00:33:18] 🌐 Starting scrape at: https://www.linkedin.com/company/real-estate-people\n",
            "[2024-10-05 00:33:18] 🔍 Extracting agent information from: https://www.linkedin.com/company/real-estate-people\n",
            "[2024-10-05 00:33:19]   ✓ Found linkedin profile\n",
            "[2024-10-05 00:33:19] 🤖 Using LLM to extract detailed information...\n",
            "[2024-10-05 00:33:19] ❌ LLM error: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7bdfc2515c60>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "[2024-10-05 00:33:19] 🔍 Scanning for internal links on: https://www.linkedin.com/company/real-estate-people\n",
            "[2024-10-05 00:33:19] 📊 Found 0 relevant internal links\n",
            "[2024-10-05 00:33:19] 🏁 Scraping completed. Found 1 agents.\n",
            "[2024-10-05 00:33:19] 🌐 Starting scrape at: https://www.linkedin.com/pub/dir/Real+Estate/Agent\n",
            "[2024-10-05 00:33:19] 🔍 Extracting agent information from: https://www.linkedin.com/pub/dir/Real+Estate/Agent\n",
            "[2024-10-05 00:33:19] 🤖 Using LLM to extract detailed information...\n",
            "[2024-10-05 00:33:19] ❌ LLM error: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7bdfce144160>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "[2024-10-05 00:33:19] 🔍 Scanning for internal links on: https://www.linkedin.com/pub/dir/Real+Estate/Agent\n",
            "[2024-10-05 00:33:19] 📊 Found 0 relevant internal links\n",
            "[2024-10-05 00:33:19] 🏁 Scraping completed. Found 1 agents.\n",
            "[2024-10-05 00:33:19] 🌐 Starting scrape at: https://www.linkedin.com/pub/dir/Real/Estate\n",
            "[2024-10-05 00:33:19] 🔍 Extracting agent information from: https://www.linkedin.com/pub/dir/Real/Estate\n",
            "[2024-10-05 00:33:19] 🤖 Using LLM to extract detailed information...\n",
            "[2024-10-05 00:33:19] ❌ LLM error: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7bdfce147910>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "[2024-10-05 00:33:19] 🔍 Scanning for internal links on: https://www.linkedin.com/pub/dir/Real/Estate\n",
            "[2024-10-05 00:33:19] 📊 Found 0 relevant internal links\n",
            "[2024-10-05 00:33:19] 🏁 Scraping completed. Found 1 agents.\n",
            "[2024-10-05 00:33:19] 🌐 Starting scrape at: https://www.paperlesspipeline.com/blog/linkedin-real-estate-marketing\n",
            "[2024-10-05 00:33:19] 🔍 Extracting agent information from: https://www.paperlesspipeline.com/blog/linkedin-real-estate-marketing\n",
            "[2024-10-05 00:33:19]   ✓ Found linkedin profile\n",
            "[2024-10-05 00:33:19]   ✓ Found facebook profile\n",
            "[2024-10-05 00:33:19]   ✓ Found twitter profile\n",
            "[2024-10-05 00:33:19] 🤖 Using LLM to extract detailed information...\n",
            "[2024-10-05 00:33:19] ❌ LLM error: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7bdfc3995ea0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "[2024-10-05 00:33:19] 🔍 Scanning for internal links on: https://www.paperlesspipeline.com/blog/linkedin-real-estate-marketing\n",
            "[2024-10-05 00:33:19] 📊 Found 0 relevant internal links\n",
            "[2024-10-05 00:33:19] 🏁 Scraping completed. Found 1 agents.\n",
            "\n",
            "Extracted Agents:\n",
            "Phone: (958) 200-0415\n",
            "Social Media:\n",
            "  - linkedin: https://www.linkedin.com/in/nicole-fitzpatrick-a9294b163\n",
            "----------------------------------------\n",
            "Social Media:\n",
            "  - linkedin: https://www.linkedin.com/company/real-estate-people\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "----------------------------------------\n",
            "Social Media:\n",
            "  - linkedin: https://www.linkedin.com/in/alexander-gudim-63ba3355\n",
            "  - facebook: https://www.facebook.com/paperlesspipeline\n",
            "  - twitter: https://www.twitter.com/PPipeline\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pz7KGvcaFvtk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}